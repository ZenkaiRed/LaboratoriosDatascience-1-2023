{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3: Regresión y Clasificación\n",
    "\n",
    "### Integrantes:\n",
    "- Marcelo Céspedes / 20723668-3\n",
    "- Carlo Ramírez / 20504139-7\n",
    "- Joaquín Pinto / 20881033-2 \n",
    "\n",
    "En esta tarea consideraremos la implementación de una regresión lineal para tareas de regresión y regresión logística para tareas de clasificación. Vamos a trabajar con varios conjuntos de datos, que serán descritos brevemente antes de importarlos. También consideraremos técnicas de regularización, las cuales estudiaremos en más detalle en clases posteriores.\n",
    "\n",
    "Primero, vamos a importar algunas librerías que necesitaremos antes de continuar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T11:24:49.969405700Z",
     "start_time": "2023-09-13T11:24:45.793677800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer conjunto de datos se llama \"Seguro de Automóvil en Suecia\". Consiste en los siguientes datos:\n",
    "\n",
    "X = Número de reclamos de seguro.\n",
    "y = Pago total por todos los reclamos en miles de Coronas Suecas por zonas geográficas en Suecia.\n",
    "\n",
    "Referencia: Comité Sueco de Análisis de Prima de Riesgo en Seguros de Motor\n",
    "http://college.hmco.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/frame.html\n",
    "\n",
    "Tu primera tarea es leer el conjunto de datos y dividirlo en un conjunto de entrenamiento (70%) y un conjunto de prueba (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:18:37.463428300Z",
     "start_time": "2023-09-13T12:18:37.130756600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X      Y\n0   108  392.5\n1    19   46.2\n2    13   15.7\n3   124  422.2\n4    40  119.4\n..  ...    ...\n58    9   87.4\n59   31  209.8\n60   14   95.5\n61   53  244.6\n62   26  187.5\n\n[63 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>108</td>\n      <td>392.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19</td>\n      <td>46.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>15.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>124</td>\n      <td>422.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40</td>\n      <td>119.4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>9</td>\n      <td>87.4</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>31</td>\n      <td>209.8</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>14</td>\n      <td>95.5</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>53</td>\n      <td>244.6</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>26</td>\n      <td>187.5</td>\n    </tr>\n  </tbody>\n</table>\n<p>63 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargar datos\n",
    "data = pd.read_csv(\"03_AutoInsurSweden.txt\", sep='\\t', header=8, decimal=',')\n",
    "display(data)\n",
    "# Separar datos en X (predictores) e y (respuesta).\n",
    "# Quizás necesites usar np.newaxis.\n",
    "X = data['X'].values # Completar\n",
    "y = data['Y'].values # Completar\n",
    "\n",
    "# Solamente los casteo a float para probar.\n",
    "X = X.astype(float)\n",
    "\n",
    "X = X[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "# Dividir para validación.\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (proporción 30-70)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n",
    "### Regresión Lineal Simple\n",
    "Vamos a implementar la Regresión Lineal utilizando las ecuaciones clásicas para datos en 2D. Esto debe hacerse utilizando únicamente numpy. Tu tarea es completar la siguiente clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:18:44.207450300Z",
     "start_time": "2023-09-13T12:18:44.182337700Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleLinearRegression(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "        self.intercept_ = 0\n",
    "\n",
    "# Los cambié para que no me diera error.\n",
    "    def fit(self, X_value, y_value):\n",
    "        self.intercept_, self.coef_ = self.coefficients(X_value, y_value)\n",
    "\n",
    "    def predict(self, X_test_value):\n",
    "        predictions = []\n",
    "        # Completar esta función\n",
    "        for x in X_test_value:\n",
    "            y_predict = self.intercept_ + self.coef_ * x\n",
    "            predictions.append(y_predict)\n",
    "        return predictions\n",
    "\n",
    "    # Calcular coeficientes\n",
    "    def coefficients(self, X_value, y_value):\n",
    "        x_mean = self.__mean(X_value)\n",
    "        y_mean = self.__mean(y_value)\n",
    "        b1 = self.__covariance(X_value, y_value) / self.__variance(X_value)\n",
    "        b0 = y_mean - b1 * x_mean\n",
    "        return [b0, b1]\n",
    "\n",
    "    # Calcular el valor promedio de una lista de números\n",
    "    def __mean(self, values):\n",
    "        return np.mean(values)\n",
    "\n",
    "    \n",
    "    # Calcular la covarianza entre X e y\n",
    "    def __covariance(self, X, y):\n",
    "        return np.cov(X.squeeze(1), y.squeeze(1), ddof=0)[0][1]\n",
    "\n",
    "    # Calcular la varianza de una lista de números\n",
    "    def __variance(self, values):\n",
    "        return np.var(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probemos el modelo con el conjunto de datos de Seguro de Automóviles en Suecia. Para hacerlo, completarás el siguiente código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:18:46.218931800Z",
     "start_time": "2023-09-13T12:18:45.833691200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficiente (b1): 3.4138235600663682\n",
      "Intercepto (b0): 19.99448575911478\n",
      "Error Cuadrático Medio: 963.9567800465602\n",
      "Coeficiente R2: 0.9051157637465794\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SimpleLinearRegression()\n",
    "# Ajustar el modelo en el conjunto de entrenamiento.\n",
    "model.fit(X, y)\n",
    "# Imprimir coeficientes.\n",
    "print(\"Coeficiente (b1):\", model.coef_)\n",
    "print(\"Intercepto (b0):\", model.intercept_)\n",
    "y_pred = model.predict(X_test)# Predecir las respuestas en el conjunto de pruebas.\n",
    "\n",
    "# Evaluar los resultados en términos del MSE (error cuadrátic medio) y coeficiente R2.\n",
    "# Imprimir los resultados sobre el conjunto de pruebas.\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Error Cuadrático Medio:\", mse)\n",
    "print(\"Coeficiente R2:\", r2)\n",
    "\n",
    "# Graficar los datos con un scatter plot y luego incluir la línea predicha por la regresión lineal.\n",
    "\n",
    "plt.scatter(X, y, label='Datos de entrenamiento', color='blue')\n",
    "\n",
    "plt.plot(X_test, y_pred, label='Línea de regresión', color='red')\n",
    "\n",
    "# Decoraciones :)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando sklearn.\n",
    "\n",
    "En la práctica, rara vez, si es que alguna vez, implementarás un algoritmo desde cero. El escenario más probable es que termines utilizando una librería; para prototipos rápidos, scikit-learn (sklearn) es la elección habitual. Así que ahora implementarás la regresión lineal utilizando esta librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:29:01.979093700Z",
     "start_time": "2023-09-13T12:29:01.916525500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficiente (b1): [3.41382356]\n",
      "Intercepto (b0): [19.99448576]\n"
     ]
    }
   ],
   "source": [
    "# Usa la clase LinearRegression de la librería scikit-learn y realiza el mismo análisis anterior.\n",
    "\n",
    "modelo = LinearRegression()\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Imprimir coeficientes\n",
    "print(\"Coeficiente (b1):\", modelo.coef_[0])\n",
    "print(\"Intercepto (b0):\", modelo.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Lineal Múltiple\n",
    "\n",
    "Ahora construiremos un modelo de Regresión Lineal Múltiple, aunque primero lo probaremos en nuestros datos en 2D. Primero, necesitamos preparar nuestros datos, deberás agregar una columna llena de unos. También necesitarás dividir los datos en conjuntos de entrenamiento y prueba nuevamente. Técnicamente, podrías simplemente agregar la columna de unos a nuestros conjuntos de datos ya divididos, pero es más fácil hacerlo desde cero (aunque no muy eficiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee los datos y agrega una nueva columna llena de números 1.\n",
    "\n",
    "X = data['X'].values # Completar aqui.\n",
    "y = data['Y'].values # Completar aqui.\n",
    "\n",
    "X = X[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "# Dividir el conjunto de datos para validación.\n",
    "X_train, X_test, y_train, y_test = # Completar (proporción 70-30 de nuevo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debes implementar la clase `MultipleLinearRegression`, completa el siguiente código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleLinearRegression(object):\n",
    "    def __init__(self):\n",
    "        self.theta_ = np.array([0])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.theta_ = # Completa esta parte (usa numpy y sus funciones de álgebra lineal)\n",
    "        \n",
    "    def predict(self,X_test):\n",
    "        predictions = []\n",
    "        # Completa esta funcion.\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vectoriza el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalua MultipleLinearRegression siguiendo el mismo esquema de antes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:**\n",
    "Compare todos los modelos implementados y explique sus diferencias si es que las hay. Debes escribir tu respuesta en esta celda.\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización\n",
    "\n",
    "Ahora que entendemos los conceptos básicos de la regresión lineal, nos centraremos en el problema más interesante de sobreajuste, que por ahora resolveremos utilizando regularización.\n",
    "\n",
    "Veremos dos modelos de regularización: Regresión Ridge (L2) y LASSO (L1), pero en esta tarea solo implementaremos uno.0 Ten en cuenta que estas ideas se pueden aplicar a otros algoritmos, pero por simplicidad las estudiaremos utilizando la regresión lineal clásica.\n",
    "\n",
    "Fuente: https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n",
    "\n",
    "La regresión Ridge y LASSO son técnicas poderosas que se utilizan generalmente para crear modelos parsimoniosos en presencia de un \"gran\" número de características. Aquí, \"gran\" puede significar típicamente una de dos cosas:\n",
    "\n",
    "- Suficientemente grande como para aumentar la tendencia de un modelo a sobreajustar (incluso tan pocas como 10 variables pueden causar sobreajuste).\n",
    "- Suficientemente grande como para causar desafíos computacionales. Con los sistemas modernos, esta situación podría surgir en caso de millones o miles de millones de características.\n",
    "\n",
    "### Nuevos datos\n",
    "\n",
    "Ahora leeremos nuevos datos multidimensionales desde el archivo 02_Winequality.csv. Nuestro objetivo será predecir la calidad de cada vino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:40:40.771167200Z",
     "start_time": "2023-09-13T12:40:40.622085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',\n",
      "       'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',\n",
      "       'ph', 'sulphates', 'alcohol', 'quality', 'red_wine'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "wine = pd.read_csv('03_Winequality.csv', delimiter=',') # Lee el archivo 03_Winequality.csv con pandas.\n",
    "\n",
    "# Reemplazar espacios en los nombres de las columnas y convertir todas las columnas a minúsculas.\n",
    "wine.columns = wine.columns.str.lower()# Completar esta parte.\n",
    "wine.columns = wine.columns.str.replace(' ', '_')\n",
    "\n",
    "print(wine.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Ridge\n",
    "\n",
    "También conocida simplemente como Regresión Lineal Regularizada o Mínimos Cuadrados Regularizados, dependiendo de la fuente. Esta técnica es fácil de implementar, de hecho, ¡la vimos en nuestra clase sobre regresión! No utilizaremos scikit-learn para esta parte, ya que simplemente sigue la misma plantilla que antes, solo que requiere proporcionar el parámetro adicional de regularización.\n",
    "\n",
    "Todo lo que necesitamos hacer es implementar la siguiente ecuación:\n",
    "\n",
    "$$\\theta = {(X^T X + \\lambda I)}^{-1} X^T y$$\n",
    "\n",
    "Entonces, tu tarea es implementar la siguiente clase utilizando numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:41:04.644122100Z",
     "start_time": "2023-09-13T12:41:04.561944700Z"
    }
   },
   "outputs": [],
   "source": [
    "class RidgeRegression(object):\n",
    "    def __init__(self, r_lambda):\n",
    "        self.theta_ = np.array([0])\n",
    "        self.lambda_ = r_lambda\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Calcular la matriz de diseño transpuesta\n",
    "        X_transpose = X.T\n",
    "        \n",
    "        # Calcular X^T * X\n",
    "        XTX = np.dot(X_transpose, X)\n",
    "        \n",
    "        # Calcular lambda * I (matriz identidad del tamaño de XTX)\n",
    "        lambda_I = self.lambda_ * np.identity(XTX.shape[0])\n",
    "        \n",
    "        # Calcular la inversa de (X^T * X + lambda * I)\n",
    "        inverse = np.linalg.inv(XTX + lambda_I)\n",
    "        \n",
    "        # Calcular X^T * y\n",
    "        XTy = np.dot(X_transpose, y)\n",
    "        \n",
    "        # Calcular theta\n",
    "        self.theta_ = np.dot(inverse, XTy) # Completar esta parte.\n",
    "        \n",
    "    def predict(self,X_test):\n",
    "        # Completar esta parte.\n",
    "        predictions = np.dot(X_test, self.theta_)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando uses la Regresión Ridge (o otros modelos como LASSO), siempre estandariza las columnas predictoras antes de construir los modelos, ¡incluso las variables categóricas codificadas como dummy! Definiremos nuestra variable objetivo y normalizaremos las columnas que no son la variable objetivo a continuación.\n",
    "\n",
    "Recordemos la ecuación para la penalización Ridge:\n",
    "\n",
    "$$ \\text{Ridge Penalty}\\; = \\lambda\\sum_{j=0}^{n-1} \\theta_j^2$$\n",
    "\n",
    "¿Cómo se ven afectados los coeficientes $\\theta$ por la media y la varianza de tus variables? Si la media y la varianza de tus predictores $x$ son diferentes, sus respectivos coeficientes $\\beta$ se escalan con la media y la varianza de los predictores, independientemente de su poder explicativo. Esto significa que una de tus variables $x$ (por ejemplo, la acidez fija) tendrá un valor $\\beta$ mucho más pequeño que otra variable (digamos, el ácido cítrico), porque la escala de las dos variables es muy diferente. Debido a que las penalizaciones Ridge no tienen en cuenta la media y la varianza de tus predictores, el ajuste depende de ti. Estas penalizaciones solo se ven afectadas por el valor de los coeficientes.\n",
    "\n",
    "Ahora prueba tu regresión con los datos del conjunto de datos de calidad del vino. No olvides normalizar restando la media y dividiendo por la desviación estándar. No debes normalizar la variable objetivo/dependiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:41:21.246562300Z",
     "start_time": "2023-09-13T12:41:21.048242200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n0          0.142462          2.188664    -2.192664       -0.744721   0.569914   \n1          0.451001          3.281982    -2.192664       -0.597594   1.197883   \n2          0.451001          2.553104    -1.917405       -0.660648   1.026618   \n3          3.073580         -0.362411     1.660957       -0.744721   0.541370   \n4          0.142462          2.188664    -2.192664       -0.744721   0.569914   \n...             ...               ...          ...             ...        ...   \n6492      -0.783154         -0.787590    -0.197039       -0.807775  -0.486215   \n6493      -0.474615         -0.119451     0.284664        0.537383  -0.257863   \n6494      -0.551750         -0.605370    -0.885185       -0.891847  -0.429127   \n6495      -1.323097         -0.301671    -0.128224       -0.912866  -0.971463   \n6496      -0.937423         -0.787590     0.422293       -0.975920  -1.028551   \n\n      free_sulfur_dioxide  total_sulfur_dioxide   density        ph  \\\n0               -1.100055             -1.446247  1.034913  1.812950   \n1               -0.311296             -0.862402  0.701432 -0.115064   \n2               -0.874695             -1.092402  0.768128  0.258100   \n3               -0.762016             -0.986248  1.101609 -0.363840   \n4               -1.100055             -1.446247  1.034913  1.812950   \n...                   ...                   ...       ...       ...   \n6492            -0.367636             -0.420095 -1.186069  0.320294   \n6493             1.491582              0.924517  0.067819 -0.426034   \n6494            -0.029596             -0.083942 -0.719196 -1.421138   \n6495            -0.592996             -0.101635 -2.003097  0.755652   \n6496            -0.480316             -0.313942 -1.762991  0.258100   \n\n      sulphates   alcohol  quality  red_wine  intercept  \n0      0.193082 -0.915394        5  1.750055          1  \n1      0.999502 -0.580023        5  1.750055          1  \n2      0.797897 -0.580023        5  1.750055          1  \n3      0.327485 -0.580023        6  1.750055          1  \n4      0.193082 -0.915394        5  1.750055          1  \n...         ...       ...      ...       ...        ...  \n6492  -0.210128  0.593772        6 -0.571323          1  \n6493  -0.478935 -0.747709        5 -0.571323          1  \n6494  -0.478935 -0.915394        6 -0.571323          1  \n6495  -1.016548  1.935253        7 -0.571323          1  \n6496  -1.419758  1.096828        6 -0.571323          1  \n\n[6497 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed_acidity</th>\n      <th>volatile_acidity</th>\n      <th>citric_acid</th>\n      <th>residual_sugar</th>\n      <th>chlorides</th>\n      <th>free_sulfur_dioxide</th>\n      <th>total_sulfur_dioxide</th>\n      <th>density</th>\n      <th>ph</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n      <th>red_wine</th>\n      <th>intercept</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.142462</td>\n      <td>2.188664</td>\n      <td>-2.192664</td>\n      <td>-0.744721</td>\n      <td>0.569914</td>\n      <td>-1.100055</td>\n      <td>-1.446247</td>\n      <td>1.034913</td>\n      <td>1.812950</td>\n      <td>0.193082</td>\n      <td>-0.915394</td>\n      <td>5</td>\n      <td>1.750055</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.451001</td>\n      <td>3.281982</td>\n      <td>-2.192664</td>\n      <td>-0.597594</td>\n      <td>1.197883</td>\n      <td>-0.311296</td>\n      <td>-0.862402</td>\n      <td>0.701432</td>\n      <td>-0.115064</td>\n      <td>0.999502</td>\n      <td>-0.580023</td>\n      <td>5</td>\n      <td>1.750055</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.451001</td>\n      <td>2.553104</td>\n      <td>-1.917405</td>\n      <td>-0.660648</td>\n      <td>1.026618</td>\n      <td>-0.874695</td>\n      <td>-1.092402</td>\n      <td>0.768128</td>\n      <td>0.258100</td>\n      <td>0.797897</td>\n      <td>-0.580023</td>\n      <td>5</td>\n      <td>1.750055</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.073580</td>\n      <td>-0.362411</td>\n      <td>1.660957</td>\n      <td>-0.744721</td>\n      <td>0.541370</td>\n      <td>-0.762016</td>\n      <td>-0.986248</td>\n      <td>1.101609</td>\n      <td>-0.363840</td>\n      <td>0.327485</td>\n      <td>-0.580023</td>\n      <td>6</td>\n      <td>1.750055</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.142462</td>\n      <td>2.188664</td>\n      <td>-2.192664</td>\n      <td>-0.744721</td>\n      <td>0.569914</td>\n      <td>-1.100055</td>\n      <td>-1.446247</td>\n      <td>1.034913</td>\n      <td>1.812950</td>\n      <td>0.193082</td>\n      <td>-0.915394</td>\n      <td>5</td>\n      <td>1.750055</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6492</th>\n      <td>-0.783154</td>\n      <td>-0.787590</td>\n      <td>-0.197039</td>\n      <td>-0.807775</td>\n      <td>-0.486215</td>\n      <td>-0.367636</td>\n      <td>-0.420095</td>\n      <td>-1.186069</td>\n      <td>0.320294</td>\n      <td>-0.210128</td>\n      <td>0.593772</td>\n      <td>6</td>\n      <td>-0.571323</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6493</th>\n      <td>-0.474615</td>\n      <td>-0.119451</td>\n      <td>0.284664</td>\n      <td>0.537383</td>\n      <td>-0.257863</td>\n      <td>1.491582</td>\n      <td>0.924517</td>\n      <td>0.067819</td>\n      <td>-0.426034</td>\n      <td>-0.478935</td>\n      <td>-0.747709</td>\n      <td>5</td>\n      <td>-0.571323</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6494</th>\n      <td>-0.551750</td>\n      <td>-0.605370</td>\n      <td>-0.885185</td>\n      <td>-0.891847</td>\n      <td>-0.429127</td>\n      <td>-0.029596</td>\n      <td>-0.083942</td>\n      <td>-0.719196</td>\n      <td>-1.421138</td>\n      <td>-0.478935</td>\n      <td>-0.915394</td>\n      <td>6</td>\n      <td>-0.571323</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6495</th>\n      <td>-1.323097</td>\n      <td>-0.301671</td>\n      <td>-0.128224</td>\n      <td>-0.912866</td>\n      <td>-0.971463</td>\n      <td>-0.592996</td>\n      <td>-0.101635</td>\n      <td>-2.003097</td>\n      <td>0.755652</td>\n      <td>-1.016548</td>\n      <td>1.935253</td>\n      <td>7</td>\n      <td>-0.571323</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6496</th>\n      <td>-0.937423</td>\n      <td>-0.787590</td>\n      <td>0.422293</td>\n      <td>-0.975920</td>\n      <td>-1.028551</td>\n      <td>-0.480316</td>\n      <td>-0.313942</td>\n      <td>-1.762991</td>\n      <td>0.258100</td>\n      <td>-1.419758</td>\n      <td>1.096828</td>\n      <td>6</td>\n      <td>-0.571323</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>6497 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Elige una variable objetivo/dependiente para predecir.\n",
    "target = 'quality'\n",
    "\n",
    "# Selecciona todas las columnas que no son la variable objetivo.\n",
    "nc = [col for col in wine.columns if col != target] # Completar aquí.\n",
    "\n",
    "# Normaliza las variables que no son el objetivo.\n",
    "# Al restar la media y dividir por la desviación estándar, el procedimiento de normalización coloca\n",
    "# todas las variables predictoras en la misma escala (distribuciones con media == 0 y desviación estándar == 1).\n",
    "wine.loc[:, nc] = (wine[nc] - wine[nc].mean()) / wine[nc].std() # Completar\n",
    "\n",
    "# Añada la columna intercepto.\n",
    "wine['intercept'] = 1 # Completar\n",
    "\n",
    "# Muestra el conjunto de datos normalizado.\n",
    "display(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:41:38.376538400Z",
     "start_time": "2023-09-13T12:41:38.260053600Z"
    }
   },
   "outputs": [],
   "source": [
    "X = wine[nc].values # Construye la matriz de características. Considera usar el atributo .values de las dataframes de pandas.\n",
    "y = wine[target].values # Construye el vector de salidas. Considera usar el atributo .values de las dataframes de pandas.\n",
    "\n",
    "# Dividir el conjunto de datos para validación.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # Proporcion 70-30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:41:50.126620900Z",
     "start_time": "2023-09-13T12:41:50.056934800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes del modelo:\n",
      "[-0.03120419 -0.29091038 -0.07426497  0.13250572  0.00556388  0.14750674\n",
      "  0.00215676 -0.10092186 -0.04911731  0.12282788  0.39699734  0.25289125]\n",
      "\n",
      "Mean Squared Error (MSE): 34.38\n",
      "R-squared (R2): -44.85\n"
     ]
    }
   ],
   "source": [
    "# Prueba tu modelo con r_lambda = 1.0\n",
    "model = RidgeRegression(1.0) # Completar aquí.\n",
    "\n",
    "# Entrena tu modelo con los datos de entrenamiento.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Imprime los coeficientes\n",
    "print(\"Coeficientes del modelo:\")\n",
    "print(model.theta_)\n",
    "\n",
    "# Predice los resultados sobre el conjunto de prueba.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcula las métricas de evaluación MSE y R2 e imprime.\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nMean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compara tus resultados anteriores con la implementación de scikit-learn. Deberías obtener los mismos resultados, ten en cuenta que $\\lambda$ se llama $\\alpha$ en scikit-learn, esto se hace en varios modelos para evitar el problema con la palabra clave ``lambda`` en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:42:16.312414Z",
     "start_time": "2023-09-13T12:42:16.233779200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados del modelo de scikit-learn:\n",
      "Mean Squared Error (MSE): 34.38\n",
      "R-squared (R2): -44.85\n"
     ]
    }
   ],
   "source": [
    "# Haz el mismo análisis con el modelo de ridge regression de scikit-learn.\n",
    "from sklearn.linear_model import Ridge\n",
    "# Entrena el modelo de Ridge de scikit-learn\n",
    "clf = Ridge(alpha=1.0, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_sklearn = clf.predict(X_test)\n",
    "\n",
    "# Calcula las métricas de evaluación para el modelo de scikit-learn 0.53, -1.25\n",
    "mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "r2_sklearn = r2_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(\"\\nResultados del modelo de scikit-learn:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_sklearn:.2f}\")\n",
    "print(f\"R-squared (R2): {r2_sklearn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu modelo debería haber arrojado un valor negativo de $R^2$. Por favor, proporciona una explicación adecuada sobre el significado de este valor.\n",
    "\n",
    "#### Respuesta:\n",
    "Tu respuesta va a aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con Regresión Logística\n",
    "\n",
    "Finalmente, nos dirigimos a las técnicas de clasificación. Ten en cuenta que la Regresión Logística en realidad proporciona una estimación de una probabilidad entre 0 y 1. Por lo tanto, es en realidad un método de regresión para ese intervalo; sin embargo, se convierte en un esquema de clasificación al interpretar esta probabilidad como la posibilidad de que un cierto punto de datos pertenezca a una clase positiva. En otras palabras, todos los valores iguales o superiores a 0.5 se asignan a 1 y todos los demás se asignan a 0 (es decir, redondeo).\n",
    "\n",
    "Utilizaremos el conjunto de datos de calidad del vino, pero en lugar de predecir la calidad, intentaremos predecir si es vino tinto o no. Inicialmente, utilizaremos la implementación de scikit-learn para mostrar lo sencillo que es. Importemos los módulos relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:37:48.229856100Z",
     "start_time": "2023-09-13T12:37:48.202326600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora lee los datos nuevamente (o revierte la normalización y cambia la variable de salida a `red_wine` en lugar de `quality`, pero eso podría llevar más tiempo). Luego, divide nuevamente en conjuntos de entrenamiento y prueba (70/30 de división)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:38:22.947217300Z",
     "start_time": "2023-09-13T12:38:22.582928800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  # Importa StandardScaler\n",
    "\n",
    "# Leer el conjunto de datos\n",
    "wine = pd.read_csv(\"03_Winequality.csv\")\n",
    "\n",
    "# Seleccionar la variable objetivo (dependiente)\n",
    "target = 'red_wine'\n",
    "\n",
    "# Seleccionar todas las columnas que no son la variable objetivo\n",
    "non_target_cols = [col for col in wine.columns if col != target]\n",
    "\n",
    "# Normalizar las variables que no son el objetivo utilizando StandardScaler\n",
    "scaler = StandardScaler()\n",
    "wine[non_target_cols] = scaler.fit_transform(wine[non_target_cols])\n",
    "\n",
    "# Agregar una columna de intercepto (unos) al conjunto de datos\n",
    "wine['intercept'] = 1\n",
    "\n",
    "# Construir la matriz de características (X) y el vector de salidas (y)\n",
    "X = wine[non_target_cols + ['intercept']].values\n",
    "y = wine[target].values\n",
    "\n",
    "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba (proporción 70-30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probemos la implementación de Regresión Logística de scikit-learn. Ten en cuenta que esta implementación incluye varios parámetros predeterminados que están ocultos (¡tiene un término de regularización por defecto!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:38:41.826391900Z",
     "start_time": "2023-09-13T12:38:41.620053300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo de Regresión Logística (scikit-learn): 0.9892307692307692\n"
     ]
    }
   ],
   "source": [
    "# Inicializar y entrenar el modelo de Regresión Logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo de Regresión Logística (scikit-learn):\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora implementemos la Regresión Logística nosotros mismos utilizando el Descenso de Gradiente. Para hacer esto, necesitamos definir nuestra función de costo:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{N}{y^{(i)}\\log(h_\\theta(x^{(i)}) + (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)})}$$\n",
    "\n",
    "#### ===== Opcional =====\n",
    "Demuestra que la derivada de la función de costo está dada por:\n",
    "\n",
    "$$ \\nabla J(\\theta) = x(\\hat{y} - y)$$\n",
    "\n",
    "Donde $y$ es la etiqueta de clase real (0 o 1), $\\hat{y}$ es la predicción del modelo (nota que esto usa una función sigmoide, que es posible que necesites) y $x$ es tu característica o vector de características). Para este ejercicio, podría ser útil investigar primero la derivada de la función sigmoide.\n",
    "#### ===== Fin de la Parte Opcional ===== "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar el Descenso de Gradiente, primero necesitaremos implementar la función sigmoide, que está definida como:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación con Descenso de Gradiente\n",
    "Ahora implementaremos la Regresión Logística utilizando el Descenso de Gradiente, completa la siguiente clase. Tienes las funciones básicas `fit` y `predict`, pero también tienes una función auxiliar que podrías necesitar para el descenso de gradiente. Ten en cuenta que $\\alpha$ es nuestra tasa de aprendizaje en este contexto (consulta las clases para obtener detalles sobre el algoritmo de Descenso de Gradiente).\n",
    "\n",
    "Para fines de nuestro ejercicio, detendremos el descenso de gradiente después de haber alcanzado un cierto número de iteraciones; no usaremos una estimación del error para detenernos. Aunque sería lo correcto, volveremos al Descenso de Gradiente más adelante, por lo que por ahora un simple bucle `for` es suficiente (nota que de otra manera tendríamos que implementar un bucle `while` y definir una tolerancia para el error, además del número máximo de iteraciones permitidas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:39:08.553479600Z",
     "start_time": "2023-09-13T12:39:08.501655900Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionGD(object):\n",
    "    def __init__(self):\n",
    "        self.theta_ = np.array([0])\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def update_weights(self, features, labels, alpha):\n",
    "        n = features.shape[1]  # Cantidad de columnas\n",
    "        self.theta_ = np.zeros(n)  # Inicializa theta con ceros\n",
    "\n",
    "        for _ in range(5000):  # Número máximo de iteraciones\n",
    "            # 1 - Obtén las predicciones\n",
    "            predictions = self.sigmoid(np.dot(features, self.theta_))\n",
    "\n",
    "            # 2 - Calcula el gradiente\n",
    "            error = labels - predictions\n",
    "            gradient = np.dot(features.T, error)\n",
    "\n",
    "            # 3 - Calcula el promedio de la derivada del costo para cada característica (dividir por n)\n",
    "            gradient /= n\n",
    "\n",
    "            # 4 - Multiplica el gradiente por la tasa de aprendizaje\n",
    "            gradient *= alpha\n",
    "\n",
    "            # 5 - Substrae el valor obtenido de los pesos para minimizar el costo\n",
    "            self.theta_ += gradient\n",
    "\n",
    "    def fit(self, X, y, max_iters=5000, lr=0.01):\n",
    "        self.update_weights(X, y, lr)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Obtén las predicciones. Este es un problema de clasificación binaria.\n",
    "        # Debes predecir 1 o 0 utilizando un umbral de 0.5.\n",
    "        probabilities = self.sigmoid(np.dot(X_test, self.theta_))\n",
    "        predictions = (probabilities >= 0.5).astype(int)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora prueba el modelo con los mismos datos que antes y compara los resultados, deberían ser similares. Puede ser necesario cambiar el número de épocas (iteraciones) para obtener resultados óptimos. Es posible que veas resultados diferentes, ya que, como mencionamos antes, la función `LogisticRegression` de scikit-learn tiene muchos parámetros adicionales inicializados en valores predeterminados. Sin embargo, los resultados aún deberían ser decentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T12:39:27.384555600Z",
     "start_time": "2023-09-13T12:39:24.161144700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo de Regresión Logística con Descenso de Gradiente: 0.9902564102564102\n"
     ]
    }
   ],
   "source": [
    "# Instancia y entrena el modelo de Regresión Logística con Descenso de Gradiente\n",
    "logistic_regression_gd = LogisticRegressionGD()\n",
    "logistic_regression_gd.fit(X_train, y_train, max_iters=10000, lr=0.01)\n",
    "\n",
    "# Realiza predicciones en el conjunto de prueba\n",
    "y_pred_gd = logistic_regression_gd.predict(X_test)\n",
    "\n",
    "# Calcula la precisión del modelo implementado con Descenso de Gradiente\n",
    "accuracy_gd = accuracy_score(y_test, y_pred_gd)\n",
    "print(\"Precisión del modelo de Regresión Logística con Descenso de Gradiente:\", accuracy_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¡Felicitaciones, has completado tu tercer tarea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
