"id","paper_id","title","url","abstract","topic"
"1","1248800.1248811","Multiobjective Optimization in Bioinformatics and Computational Biology","http://dl.acm.org/citation.cfm?id=1248800.1248811","This paper reviews the application of multiobjective optimization in the fields of bioinformatics and computational biology. A survey of existing work, organized by application area, forms the main body of the review, following an introduction to the key concepts in multiobjective optimization. An original contribution of the review is the identification of five distinct 'contexts, giving rise to multiple objectives: These are used to explain the reasons behind the use of multiobjective optimization in each application area and also to point the way to potential future uses of the technique.","bioinformatics"
"2","1762370.1762416","Integrating mutations data of the TP53 human gene in the bioinformatics network environment","http://dl.acm.org/citation.cfm?id=1762370.1762416","We present in this paper some new network tools that can improve the accessibility of information on mutations of the TP53 human gene with the aims of allowing for the integration of this data in the growing bioinformatics network environment and of demonstrating a possible methodology for biological data integration. We implemented the IARC TP53 Mutations Database and related subsets in an SRS site, set up some Web Services allowing for a software oriented, programmatic access to this data, created some demo workflows that illustrate how to interact with Web Services and implemented these workflows in the biowep (Workflow Enactment Portal for Bioinformatics) system. In conclusion, we discuss a new flexible and adaptable methodology for data integration in the biomedical research application domain.","bioinformatics"
"3","1356536.1356539","The Grid ontology: bioinformatics service discovery","http://dl.acm.org/citation.cfm?id=1356536.1356539","myGrid supports in silico experiments in the life sciences, enabling the design and enactment of workflows as well as providing components to assist service discovery, data and metadata management. The myGrid ontology is one component in a larger semantic discovery framework for the identification of the highly distributed and heterogeneous bioinformatics services in the public domain. From an initial model of formal OWL-DL semantics throughout, we now adopt a spectrum of expressivity and reasoning for different tasks in service annotation and discovery. Here, we discuss the development and use of the myGrid ontology and our experiences in semantic service discovery.","bioinformatics"
"4","1356536.1356537","An ontology-based framework for bioinformatics workflows","http://dl.acm.org/citation.cfm?id=1356536.1356537","The proliferation of bioinformatics activities brings new challenges how to understand and organise these resources, how to exchange and reuse successful experimental procedures, and to provide interoperability among data and tools. This paper describes an effort toward these directions. It is based on combining research on ontology management, AI and scientific workflows to design, reuse and annotate bioinformatics experiments. The resulting framework supports automatic or interactive composition of tasks based on AI planning techniques and takes advantage of ontologies to support the specification and annotation of bioinformatics workflows. We validate our proposal with a prototype running on real data.","bioinformatics"
"5","1739216.1739219","Medical informatics: transition from data acquisition to data analysis by means of bioinformatics tools and resources","http://dl.acm.org/citation.cfm?id=1739216.1739219","Medical informatics has shifted its focus from acquisition and storage of healthcare data by integrating computational, informational, cognitive and organisational sciences to semantic analysis of the data for problem solving and clinical decision-making. In this transition, bioinformatics tools and resources are the most appropriate means to improve the analysis, as major biological databases are now containing clinical data alongside genomics, proteomics and other biological data. This paper briefly reviews bioinformatics tools and resources and then discusses their applications in analysing clinical data for diagnostics.","bioinformatics"
"6","2415656.2415667","Design and Analysis of Classifier Learning Experiments in Bioinformatics: Survey and Case Studies","http://dl.acm.org/citation.cfm?id=2415656.2415667","In many bioinformatics applications, it is important to assess and compare the performances of algorithms trained from data, to be able to draw conclusions unaffected by chance and are therefore significant. Both the design of such experiments and the analysis of the resulting data using statistical tests should be done carefully for the results to carry significance. In this paper, we first review the performance measures used in classification, the basics of experiment design and statistical tests. We then give the results of our survey over 1,500 papers published in the last two years in three bioinformatics journals (including this one). Although the basics of experiment design are well understood, such as resampling instead of using a single training set and the use of different performance metrics instead of error, only 21 percent of the papers use any statistical test for comparison. In the third part, we analyze four different scenarios which we encounter frequently in the bioinformatics literature, discussing the proper statistical methodology as well as showing an example case study for each. With the supplementary software, we hope that the guidelines we discuss will play an important role in future studies.","bioinformatics"
"7","2428271.2428284","An open framework for extensible multi-stage bioinformatics software","http://dl.acm.org/citation.cfm?id=2428271.2428284","In research labs, there is often a need to customise software at every step in a given bioinformatics workflow, but traditionally it has been difficult to obtain both a high degree of customisability and good performance. Performance-sensitive tools are often highly monolithic, which can make research difficult. We present a novel set of software development principles and a bioinformatics framework, Friedrich, which is currently in early development. Friedrich applications support both early stage experimentation and late stage batch processing, since they simultaneously allow for good performance and a high degree of flexibility and customisability. These benefits are obtained in large part by basing Friedrich on the multiparadigm programming language Scala. We present a case study in the form of a basic genome assembler and its extension with new functionality. Our architecture has the potential to greatly increase the overall productivity of software developers and researchers in bioinformatics.","bioinformatics"
"30","976520.976556","Initial SARS coronavirus genome sequence analysis using a bioinformatics platform","http://dl.acm.org/citation.cfm?id=976520.976556","A dedicated anti-SARS bioinformatics web site was setup in April 2003 at the Centre of bioinformatics (CBI), Peking University (<u>http://antisars.cbi.pku.edu.cn/</u>). A special bioinformatics platform was constructed to analyse the sequence and structure data of SARS coronavirus and other viruses. A total file of 32 SARS coronavirus genome sequences was retrieved from GenBank and mismatches in 30 sites were revealed from the result of multiple sequence alignment. The SARS coronavirus genome sequences can be divided into three groups based on the phylogenetic analysis using the data set constructed from the sequence mismatches.","bioinformatics"
"8","2189805.2189810","Fast Parallel Markov Clustering in Bioinformatics Using Massively Parallel Computing on GPU with CUDA and ELLPACK-R Sparse Format","http://dl.acm.org/citation.cfm?id=2189805.2189810","Markov clustering (MCL) is becoming a key algorithm within bioinformatics for determining clusters in networks. However, with increasing vast amount of data on biological networks, performance and scalability issues are becoming a critical limiting factor in applications. Meanwhile, GPU computing, which uses CUDA tool for implementing a massively parallel computing environment in the GPU card, is becoming a very powerful, efficient, and low-cost option to achieve substantial performance gains over CPU approaches. The use of on-chip memory on the GPU is efficiently lowering the latency time, thus, circumventing a major issue in other parallel computing environments, such as MPI. We introduce a very fast Markov clustering algorithm using CUDA (CUDA-MCL) to perform parallel sparse matrix-matrix computations and parallel sparse Markov matrix normalizations, which are at the heart of MCL. We utilized ELLPACK-R sparse format to allow the effective and fine-grain massively parallel processing to cope with the sparse nature of interaction networks data sets in bioinformatics applications. As the results show, CUDA-MCL is significantly faster than the original MCL running on CPU. Thus, large-scale parallel computation on off-the-shelf desktop-machines, that were previously only possible on supercomputing architectures, can significantly change the way bioinformaticians and biologists deal with their data.","bioinformatics"
"9","1706500.1706507","Integrating flexibility and interactivity in bioinformatics visual programming tools with Focus Context algorithm","http://dl.acm.org/citation.cfm?id=1706500.1706507","An improved bioinformatics visual programming prototype system VBP that aims to visualise highly complicated bioinformatical data is described. In this paper, we describe the integration of Focus Context algorithm and bio-visual programming to show the dynamic adjusting of focusing on details without losing the context simultaneously. Because of this flexible and interactive architecture, VBP makes an ideal bio-visual programming tool for future bioinformatics or systems biology research.","bioinformatics"
"10","1521325.1521326","Scientific Review: A new insight on predicting tumour malignancies using synergistic computational intelligence and bioinformatics approaches","http://dl.acm.org/citation.cfm?id=1521325.1521326","Recently, the National Human Genome Research Institute and National Cancer Institute, both part of NIH, US Department of Health and Human Services, have launched The Cancer Genome Atlas (TCGA). Based on the mission of TCGA, we have proposed a further parallel paradigm on cancer: it is not only the genetic changes (i.e., mutations of genes) but also changes of gene expressions and regulatory networks that are ultimately responsible for cancer development. Under this parallel paradigm, un-mutated genes with differential expressions and alternative splicing may also induce changes in the differential regulatory networks that also cause cancer when cells are subjected to unusual environments. We developed a novel synergistic computational intelligence and bioinformatics approach to predict malignancies of neuroendocrine tumours that are particularly important to discover the mechanisms of human genome mechanisms relating malignant transformation.","bioinformatics"
"11","1356595.1356600","Analysing the performance of personal computers based on Intel microprocessors for sequence aligning bioinformatics applications","http://dl.acm.org/citation.cfm?id=1356595.1356600","Aligning specific sequences against a very large number of other sequences is a central aspect of bioinformatics. With the widespread availability of personal computers in biology laboratories, sequence alignment is now often performed locally. This makes it necessary to analyse the performance of personal computers for sequence aligning bioinformatics benchmarks. In this paper, we analyse the performance of a personal computer for the popular BLAST and FASTA sequence alignment suites. Results indicate that these benchmarks have a large number of recurring operations and use memory operations extensively. It seems that the performance can be improved with a bigger L1-cache.","bioinformatics"
"12","820189.820218","An empirical comparison of supervised machine learning techniques in bioinformatics","http://dl.acm.org/citation.cfm?id=820189.820218","Research in bioinformatics is driven by the experimental data. Current biological databases are populated by vast amounts of experimental data. Machine learning has been widely applied to bioinformatics and has gained a lot of success in this research area. At present, with various learning algorithms available in the literature, researchers are facing difficulties in choosing the best method that can apply to their data. We performed an empirical study on 7 individual learning systems and 9 different combined methods on 4 different biological data sets, and provide some suggested issues to be considered when answering the following questions: (i) How does one choose which algorithm is best suitable for their data set? (ii) Are combined methods better than a single approach? (iii) How does one compare the effectiveness of a particular algorithm to the others?","bioinformatics"
"13","2147805.2147881","Gaussian logic and its applications in bioinformatics","http://dl.acm.org/citation.cfm?id=2147805.2147881","We describe a novel statistical relational learning framework capable to work efficiently with combinations of relational and numerical data which is especially valuable in bioinformatics applications. We show how this model can be applied to modelling of gene expression data and to problems from proteomics.","bioinformatics"
"14","2649387.2660852","A taxonomy for bioinformatics tools: exploiting semantics, parallelism, and services for analyzing omics data","http://dl.acm.org/citation.cfm?id=2649387.2660852","The overwhelming availability of omics data poses new challenges to bioinformatics applications, such as the efficient storage, preprocessing, and analysis of experimental data; the building of reproducible 'in silico' experiments; the annotation of omics data with terms extracted from ontologies; and the integration of omics and clinical data. We present BioTax, a taxonomy for bioinformatics tools, and its web-based system that users can access either to query the taxonomy and retrieve metadata about bioinformatics tools, or to submit and classify new bioinformatics tools according to the taxonomy itself. BioTax is implemented by using the OWL Web Ontology Language and currently it classifies 14 bioinformatics tools. BioTax can help both decision makers as well as application designers in choosing bioinformatics tools during the development of complex biomedical applications and in better understanding their strengths and weaknesses in the various stages of the bioinformatics analysis pipeline.","bioinformatics"
"15","2081752.2081760","Bioinformatics data source integration based on semantic relationships across species","http://dl.acm.org/citation.cfm?id=2081752.2081760","Bioinformatics databases are heterogeneous, differ in their representation as well as in their query capabilities across diverse information held in distributed autonomous resources. Current approaches to integrating heterogeneous bioinformatics data sources are based on one of a: common field, ontology or cross-reference. In this paper we investigate the use of semantic relationships across species to link, integrate and annotate genes from publicly available data sources and a novel Soft Link approach is introduced, to link information across species held in biological databases, through providing a flexible method of joining related information from different databases, including non-bioinformatics databases. A measure of relationship closeness will afford a biologist a new tool in their repertoire for analysis. Soft Links are identified as interrelated concepts and can be used to create a rich set of possible relation types supporting the investigation of alternative hypothesis.","bioinformatics"
"16","1843144.1843155","Cache-Oblivious Dynamic Programming for Bioinformatics","http://dl.acm.org/citation.cfm?id=1843144.1843155","We present efficient cache-oblivious algorithms for some well-studied string problems in bioinformatics including the longest common subsequence, global pairwise sequence alignment and three-way sequence alignment (or median), both with affine gap costs, and RNA secondary structure prediction with simple pseudoknots. For each of these problems, we present cache-oblivious algorithms that match the best-known time complexity, match or improve the best-known space complexity, and improve significantly over the cache-efficiency of earlier algorithms. We present experimental results which show that our cache-oblivious algorithms run faster than software and implementations based on previous best algorithms for these problems.","bioinformatics"
"17","2687034.2687043","Maximum likelihood estimation of GEVD: applications in bioinformatics","http://dl.acm.org/citation.cfm?id=2687034.2687043","We propose a method, maximum likelihood estimation of generalized eigenvalue decomposition (MLGEVD) that employs a well known technique relying on the generalization of singular value decomposition (SVD). The main aim of the work is to show the tight equivalence between MLGEVD and generalized ridge regression. This relationship reveals an important mathematical property of GEVD in which the second argument act as prior information in the model. Thus we show that MLGEVD allows the incorporation of external knowledge about the quantities of interest into the estimation problem. We illustrate the importance of prior knowledge in clinical decision making/identifying differentially expressed genes with case studies for which microarray data sets with corresponding clinical/literature information are available. On all of these three case studies, MLGEVD outperformed GEVD on prediction in terms of test area under the ROC curve (test AUC). MLGEVD results in significantly improved diagnosis, prognosis and prediction of therapy response.","bioinformatics"
"18","2032098.2032121","A knowledge based decision support system for bioinformatics and system biology","http://dl.acm.org/citation.cfm?id=2032098.2032121","In this paper, we present a new Decision Support System for Bioinformatics and System Biology issues. Our system is based on a Knowledge base, representing the expertise about the application domain, and a Reasoner. The Reasoner, consulting the Knowledge base and according to the user's request, is able to suggest one or more strategies in order to resolve the selected problem. Moreover, the system can build, at different abstraction layers, a workflow for the current problem on the basis of the user's choices, freeing the user from implementation details and assisting him in the correct configuration of the algorithms. Two possible application scenarios will be introduced: the analysis of protein-protein interaction networks and the inference of gene regulatory networks.","bioinformatics"
"19","2165590.2165592","Pattern recognition in bioinformatics: an introduction","http://dl.acm.org/citation.cfm?id=2165590.2165592","The information stored in DNA, a chain of four nucleotides (A, T, G, and C), is first converted to mRNA through the process of transcription and then converted to the functional form of life, proteins, through the process of translation. Only about 5% of the genome contains useful patterns of nucleotides, or genes, that code for proteins. The initiation of translation or transcription process is determined by the presence of specific patterns of DNA or RNA, or motifs. Research on detecting specific patterns of DNA sequences such as genes, protein coding regions, promoters, etc., leads to uncover functional aspects of cells. Comparative genomics focus on comparisons across the genomes to find conserved patterns over the evolution, which possess some functional significance. Construction of evolutionary trees is useful to know how genome and proteome are evolved over all species by ways of a complete library of motifs and genes.","bioinformatics"
"20","1759681.1759707","Extensions of naive bayes and their applications to bioinformatics","http://dl.acm.org/citation.cfm?id=1759681.1759707","In this paper we will study the nave Bayes, one of the popular machine learning algorithms, and improve its accuracy without seriously affecting its computational efficiency. Nave Bayes assumes positional independence, which makes the computation of the joint probability value easier at the expense of the accuracy or the underlying reality. In addition, the prior probabilities of positive and negative instances are computed from the training instances, which often do not accurately reflect the real prior probabilities. In this paper we address these two issues. We have developed algorithms that automatically perturb the computed prior probabilities and search around the neighborhood to maximize a given objective function. To improve the prediction accuracy we introduce limited dependency on the underlying pattern. We have demonstrated the importance of these extensions by applying them to solve the problem in discriminating a TATA box from putative TATA boxes found in promoter regions of plant genome. The best prediction accuracy of a nave Bayes with 10 fold cross validation was 69% while the second extension gave the prediction accuracy of 79% which is better than the best solution from an artificial neural network prediction.","bioinformatics"
"21","1880631.1880647","Basics of game theory for bioinformatics","http://dl.acm.org/citation.cfm?id=1880631.1880647","In this 'tutorial' it is offered a quick introduction to game theory and to some suggested readings on the subject. It is also considered a small set of game theoretical applications in the bioinformatics field.","bioinformatics"
"22","1893880.1893889","Enabling annotation provenance in bioinformatics workflow applications","http://dl.acm.org/citation.cfm?id=1893880.1893889","Nowadays, the resulting data of each step of genomic annotation processes are typically used for annotation provenance. However, recent initiatives in the direction of capturing annotation provenance data do not support the Bioinformatics developer on building their own provenance enabled applications. This work proposes the ArCaP architecture, which aims at supporting the development of provenance enabled applications and at facilitating the access to annotation provenance data. Although the focus is on Bioinformatics applications, this approach is useful to other scientific domains.","bioinformatics"
"23","2649387.2649429","A Hadoop-Galaxy adapter for user-friendly and scalable data-intensive bioinformatics in Galaxy","http://dl.acm.org/citation.cfm?id=2649387.2649429","In this work we present a strategy to integrate Hadoop-based applications into the Galaxy platform along with an extensible implementation of this adapter and related utilities. The strategy is based on the idea of introducing a new Galaxy datatype that provides a layer of indirection, thus relaxing the requirement to place data on a Galaxy-accessible file system and instead allowing the referenced data to be placed on any addressable space, including the Hadoop Distributed File System or Amazon S3. The adapter supports using Hadoop-based applications as part of Galaxy workflows. We demonstrate a practical application where this Hadoop-Galaxy adapter was used at CRS4 to accelerate the bioinformatics analysis of viral vector integration sites through the introduction of Hadoop-based computation components, while keeping the workflow under control of biologists with little specific technical training.","bioinformatics"
"24","1537772.1537774","Association Analysis Techniques for Bioinformatics Problems","http://dl.acm.org/citation.cfm?id=1537772.1537774","Association analysis is one of the most popular analysis paradigms in data mining. Despite the solid foundation of association analysis and its potential applications, this group of techniques is not as widely used as classification and clustering, especially in the domain of bioinformatics and computational biology. In this paper, we present different types of association patterns and discuss some of their applications in bioinformatics. We present a case study showing the usefulness of association analysis-based techniques for pre-processing protein interaction networks for the task of protein function prediction. Finally, we discuss some of the challenges that need to be addressed to make association analysis-based techniques more applicable for a number of interesting problems in bioinformatics.","bioinformatics"
"25","1854776.1854864","Hunting for truly relevant articles in bioinformatics literature: a preliminary study","http://dl.acm.org/citation.cfm?id=1854776.1854864","For researchers interested in reading articles concerning a specific topic, the current document search techniques, based primarily on keyword matching, are insufficient. They tend to return too many 'hits', most of which are not truly relevant. An individualized text filtering system that can select/recommend useful articles would be a tremendous time-saver for researchers, especially in the field of bioinformatics, in which numerous articles are published daily. Machine learning tools such as text classification may be the answer to this need. This paper describes some preliminary work on developing such a text filtering system. Support Vector Machine is used to classify articles from Journal of Bacteriology to determine whether an article addresses issues related to 'gene function'. Preliminary results, problems, and difficulties encountered are discussed.","bioinformatics"
"27","1614645.1614665","Identification of Proteins from Tuberculin Purified Protein Derivative (PPD) with Potential for TB Diagnosis Using Bioinformatics Analysis","http://dl.acm.org/citation.cfm?id=1614645.1614665","The PPD is widely used for diagnosis of bovine tuberculosis, however it lacks specificity and sensitivity, generally attributed the cross-reactions with antigens shared by other <em>Mycobacterium</em> species. These highlight the necessity of better diagnostic tools to detect tuberculosis in cattle. We have identified proteins present in PPD preparations (avium and bovine PPD) by LC-MS/MS (Liquid Chromatography/Mass Spectrometry/Mass Spectrometry). A total of 169 proteins were identified. From these, four PPD proteins identified in bovine PPD and absent in avium PPD (Mb1961c, Mb2898, Mb2900 and Mb2965c) were select for bioinformatics analysis to identify an antigen with potential for TB diagnosis. We identified Mb1961c, a secreted protein, which has low identity proteins found in environmental mycobacteria. This protein could be used for the development of a diagnostic test or a synthetic PPD for bovine tuberculosis.","bioinformatics"
"28","2718771.2718772","Meta-learning framework applied in bioinformatics inference system design","http://dl.acm.org/citation.cfm?id=2718771.2718772","This paper describes a meta-learner inference system development framework which is applied and tested in the implementation of bioinformatic inference systems. These inference systems are used for the systematic classification of the best candidates for inclusion in bacterial metabolic pathway maps. This meta-learner-based approach utilises a workflow where the user provides feedback with final classification decisions which are stored in conjunction with analysed genetic sequences for periodic inference system training. The inference systems were trained and tested with three different data sets related to the bacterial degradation of aromatic compounds. The analysis of the meta-learner-based framework involved contrasting several different optimisation methods with various different parameters. The obtained inference systems were also contrasted with other standard classification methods with accurate prediction capabilities observed.","bioinformatics"
"29","820189.820194","Oracle's technology for bioinformatics and future directions","http://dl.acm.org/citation.cfm?id=820189.820194","The Oracle relational database management system, with object-oriented extensions and numerous application-driven enhancements, plays a critical role worldwide in managing the exploding volumes of bioinformatics data. There are many features of the Oracle product which support the bioinformatics community directly already and there are several features that could be exploited more thoroughly by users, service vendors, and Oracle itself to extend that level of support. This paper will present an overview of Oracle features that support storage of bioinformatics data and will discuss extensibility features that give the product room to grow. Some attention will be given to Oracle's own efforts to use that extensibility to make commodities of many of the complex data and computation requirements of the life sciences.","bioinformatics"
"31","1544177.1544195","Parallel Clustering Algorithm for Large Data Sets with Applications in Bioinformatics","http://dl.acm.org/citation.cfm?id=1544177.1544195","Large sets of bioinformatical data provide a challenge in time consumption while solving the cluster identification problem, and that is why a parallel algorithm is so needed for identifying dense clusters in a noisy background. Our algorithm works on a graph representation of the data set to be analyzed. It identifies clusters through the identification of densely intraconnected subgraphs. We have employed a minimum spanning tree (MST) representation of the graph and solve the cluster identification problem using this representation. The computational bottleneck of our algorithm is the construction of an MST of a graph, for which a parallel algorithm is employed. Our high-level strategy for the parallel MST construction algorithm is to first partition the graph, then construct MSTs for the partitioned subgraphs and auxiliary bipartite graphs based on the subgraphs, and finally merge these MSTs to derive an MST of the original graph. The computational results indicate that when running on 150 CPUs, our algorithm can solve a cluster identification problem on a data set with 1,000,000 data points almost 100 times faster than on single CPU, indicating that this program is capable of handling very large data clustering problems in an efficient manner. We have implemented the clustering algorithm as the software CLUMP.","bioinformatics"
"32","820189.820190","From informatics to bioinformatics","http://dl.acm.org/citation.cfm?id=820189.820190","Informatics has helped in launching molecular biology into the genomic era. It appears certain that informatics will continue to be a major factor in the success of molecular biology in the post-genome era. In this paper, we describe advances made in data integration and data mining technologies that are relevant to molecular biology and biomedical sciences. In particular, we discuss some past and present research results on topics such as (a) the taming of autonomous heterogeneous distributed data sources, (b) the prediction of immunogenic peptides, (c) the discovery of gene structure features, (d) the classification of gene expression profiles, and (e) the extraction of protein interaction information from literature.","bioinformatics"
"33","1779957.1779974","Bioinformatics on -barrel membrane proteins: sequence and structural analysis, discrimination and prediction","http://dl.acm.org/citation.cfm?id=1779957.1779974","The analysis on the amino acid sequences of transmembrane beta barrel proteins (TMBs) provides deep insights about their structure and function. We found that the occurrence of Ser, Asn and Gln is significantly higher in TMBs than globular proteins, which might be due to their importance in the formation of -barrel structures in the membrane, stability of binding pockets and the function of TMBs. Utilizing this information, we have devised statistical methods and machine learning techniques to discriminate TMBs from other folding types of globular and membrane proteins and we obtained the maximum accuracy of 96%. Further, we have devised protocols for identifying the membrane spanning -strand segments and detecting TMBs in genomic sequences.","bioinformatics"
"34","1573222.1573421","Intuitive Bioinformatics for Genomics Applications: Omega-Brigid Workflow Framework","http://dl.acm.org/citation.cfm?id=1573222.1573421","The recent developments in life sciences and technology have produced large amounts of data in an extremely fast and cost-efficient way which require the development of new algorithms, coupled with massively parallel computing. Besides, biologists are usually non-programmers, thus demanding intuitive computer applications that are easy to use by means of a friendly GUI. In addition, different algorithms, databases and other tools usually lie on incompatible file formats, applications, operating systems and hardware platforms. It is therefore of paramount importance to overcome such limitations, so that bioinformatics becomes much more widely used amongst biologists. The main goal of our research project is to unify many of these existing bioinformatics applications and resources (local and remote) in one easy-to-use environment, independent of the computing platform, being a concentrator resource tool with a friendly interface. To achieve this, we propose a tool based on a new, open, free and well-documented architecture called Biomniverso. Two main elements make up such a tool: its kernel (Omega), which supplies services specifically adapted to allow the addition of new bioinformatics functionalities by means of plugins (like Minerva, which makes easy to detect SNP amongst a set of genomic data to discover fraudulent olive oil), and the interface (Brigid), which allows even non-programmer laboratory scientists to chain different processes into workflows and customize them without code writing.","bioinformatics"
"35","2447509.2447510","Meropenem: a potent drug against superbug as unveiled through bioinformatics approaches","http://dl.acm.org/citation.cfm?id=2447509.2447510","Global spread of multi-drug resistant bacteria like Klebsiella pneumoniae and Escherichia coli have raised the alarm for researchers and doctors throughout the world. This new mechanism of resistance and the ability of ndm-1 gene to be transferred between the species may end the era of antibiotics treatment. Carbapenems are reliable drugs against many multi-resistant gram-negative pathogens. A 3-D homology model of NDM-1 was built and analysed for elucidation of functional site and binding interactions. This study revealed that meropenem has good interaction with the active sites of the receptor that could retard the spread of antibiotic resistant bacteria.","bioinformatics"
"36","1651318.1651320","Data mining in bioinformatics: challenges and opportunities","http://dl.acm.org/citation.cfm?id=1651318.1651320","In this talk I will discuss some data mining techniques and methods in the bioinformatics domain, what are the main challenges and what are the opportunities. I will cover some of the issues related to biomedical literature mining, bioinformatics data integration and biological network analysis and simulation. In biomedical literature mining, I will discuss the effective information retrieval and large-scale information extraction from biomedical literatures. I will also share my view of the semantic-based approach for data integration for bioinformatics domain. In the end, I will talk about the various approaches for biological network analysis and simulation.","bioinformatics"
"37","1963526.1963530","Predictions of flexible C-terminal tethers of bacterial proteins with the FLEXTAIL bioinformatics pipeline","http://dl.acm.org/citation.cfm?id=1963526.1963530","Proteins use conserved binding motifs associated with relatively unconserved flexible amino acid sequences as mobile tethers for interacting molecules, as exemplified by C-terminal sequences of bacterial chemotaxis receptors. The FLEXTAIL bioinformatics pipeline predicts flexible tethers and their binding motifs based on the properties of flexibility and sequence conservation. In four groups of bacterial genomes, the algorithm identified  100 putative binding domains, including verifying the known bacterial chemotaxis receptor   NWETF binding motif. Some potential C-terminal flexible regions that have not previously been recognised to function as protein tethers were found and should be investigated further for binding targets and flexibility.","bioinformatics"
"38","2649387.2660796","High-performance recursive dynamic programming for bioinformatics using MM-like flexible kernels","http://dl.acm.org/citation.cfm?id=2649387.2660796","Dynamic Programming (DP) provides optimal solutions to a problem by combining optimal solutions to many overlapping subproblems. DP algorithms exploit this overlapping property to explore otherwise exponential-sized problem spaces in polynomial time, making them central to many important applications spanning from logistics to computational biology. In this paper, we present a general strategy of obtaining highly efficient parallel DP implementations using recursive cache-oblivious divide and conquer technique which turns inflexible kernels into flexible ones (kernels that read from and write to disjoint sub-matrices). We solve four non-trivial DP problems widely used in Bioinformatics, namely the parenthesis problem, Floyd-Warshall's all-pairs shortest paths, gap problem and protein accordion folding using recursive cache-oblivious technique that decompose the original inflexible looping kernel to highly optimizable flexible kernels. To the best of our knowledge no such recursive parallel DP algorithms were known for the protein folding and gap problems. The algorithms are hybrid in the same way as most high-performance matrix multiplication algorithms are recursive with iterative base cases. We show that the base cases of these recursive divide-and-conquer algorithms are predominantly matrix-multiplication-like (MM-like) flexible that expose many optimization opportunities not offered by the traditional looping DP codes. Moreover, the most costly/dominating kernel for these problems are often flexible. As a result, one can obtain highly efficient DP implementations by simply optimizing these kernels. We present a few generic optimization steps that suffices to optimize these DP implementations. Our implementations achieve 5--100x speedup over their standard loop based DP counterparts on modern multicore machines. We also present results on manycores (Xeon Phi) and clusters of multicores obtained by simple extensions for SIMD and shared-distributed-shared-memory architectures, respectively.","bioinformatics"
"61","1232954.1233149","Towards a platform for wide-area overlay network deployment and management","http://dl.acm.org/citation.cfm?id=1232954.1233149","This paper presents an experimental set of self-organising mechanisms to support application-level service overlays across the internet. These mechanisms include a base node discovery service and protocol that is resilient to node and network failure, which in turn is a basis for the construction of service-specific overlays. We have also developed a generic service overlay structured according to IP network latencies which uses the GNP virtual coordinate system. We discuss our results and experiences of implementing our platform using an active service infrastructure deployed on PlanetLab. While we have been successful in meeting many of our aims, we have identified some practical shortcomings with regard to latency estimation accuracy, and scalability of some of our mechanisms to internet size. The paper describes lessons learned from experimental development and deployment.","network"
"62","984622.984650","Lattice sensor networks: capacity limits, optimal routing and robustness to failures","http://dl.acm.org/citation.cfm?id=984622.984650","We study network capacity limits and optimal routing algorithms for regular sensor networks, namely, square and torus grid sensor networks, in both, the static case (no node failures) and the dynamic case (node failures). For static networks, we derive upper bounds on the network capacity and then we characterize and provide optimal routing algorithms whose rate per node is equal to this upper bound, thus, obtaining the exact analytical expression for the network capacity. For dynamic networks, the unreliability of the network is modeled in two ways: a Markovian node failure and an energy based node failure. Depending on the probability of node failure that is present in the network, we propose to use a particular combination of two routing algorithms, the first one being optimal when there are no node failures at all and the second one being appropriate when the probability of node failure is high. The combination of these two routing algorithms defines a family of randomized routing algorithms, each of them being suitable for a given probability of node failure.","network"
"63","1127777.1127813","Sleep scheduling and lifetime maximization in sensor networks: fundamental limits and optimal solutions","http://dl.acm.org/citation.cfm?id=1127777.1127813","Energy efficiency is a very critical consideration in the design of low cost sensor networks which typically have fairly low node battery life. This raises the need for providing periodic sleep cycles for the radios in the sensor nodes. Keeping sensors in sleep state also implies that node to sink communication incurs certain delays and there exists a threshold on the duty cycling for the communication delay to be bounded, giving rise to an upperbound on the lifetime of the network i.e., the time until at least one node in the network is able to communicate its sensed data to the sink. This paper aims at establishing tight analytical bounds on the sleeping probabilities of nodes and on the achievable lifetime of wireless sensor networks in a very generic setting. Bounds on the sleeping probability need to be satisfied for proper network functionality. Further, an energy efficient deployment scheme is suggested wherein the battery power depletion is fairly uniformly deployed throughout the network. This scheme makes use of the availability of low power auxiliary channel listening radio. With this scheme, we shown that an improvement in lifetime by a factor of O(n overlog n) over uniform distribution of nodes is achievable, where n is the number of nodes in the network. We also show that the throughput capacity of the network is also improved by the same factor. We show also that the maximum lifetime of the network is bounded above by O(n3/2 over log n). Further, the accuracy of our analysis is verified by the simulation results presented.","network"
"64","570738.570751","Wireless sensor networks for habitat monitoring","http://dl.acm.org/citation.cfm?id=570738.570751","We provide an in-depth study of applying wireless sensor networks to real-world habitat monitoring. A set of system design requirements are developed that cover the hardware design of the nodes, the design of the sensor network, and the capabilities for remote data access and management. A system architecture is proposed to address these requirements for habitat monitoring in general, and an instance of the architecture for monitoring seabird nesting environment and behavior is presented. The currently deployed network consists of 32 nodes on a small island off the coast of Maine streaming useful live data onto the web. The application-driven design exercise serves to identify important areas of further work in data sampling, communications, network retasking, and health monitoring.","network"
"71","984622.984686","Naps: scalable, robust topology management in wireless ad hoc networks","http://dl.acm.org/citation.cfm?id=984622.984686","Topology management schemes conserve energy in wireless ad hoc networks by identifying redundant nodes that may turn off their radios or other components while maintaining connectivity. We present Naps, a randomized topology management scheme that does not rely on geographic location information, provides exibility in the target density of waking nodes, and sends only a periodic heartbeat message between waking neighbors; thus it is implementable even on modest hardware. We formally analyze the connectivity of the waking graphs produced by Naps, showing that these graphs have nearly complete connectivity even at relatively low densities. We examine simulation results for a wide range of initial deployment densities and for heterogeneous and mobile deployments.","network"
"65","941350.941364","Role-based hierarchical self organization for wireless ad hoc sensor networks","http://dl.acm.org/citation.cfm?id=941350.941364","Efficiently self organizing a network hierarchy with specific assignment of roles (or tasks) to sensors based on their physical wireless connectivity and sensing characteristics is an important and challenging problem. In this paper, we extend the hierarchical connected dominating set (CDS) construction algorithm, proposed by Jie Wu, to develop our role-based hierarchical self organization algorithm for wireless sensor networks. The resulting self organized sensor network establishes a network-wide infrastructure consisting of a hierarchy of backbone nodes, and sensing zones that include sensor coordinators, and sensing collaborators (or sensing zone members). Our paper identifies the need for organizing a sensor network according to the tasks appropriate for each sensor node based on their initial deployment in the network. Past research in group-based (or hierarchical) sensor networks have ignored the possibility of utilizing both the physical communication and sensing characteristics to assign roles to sensor nodes. We demonstrate the effectiveness of our design, which considers both, through simulations.","network"
"66","941350.941369","Throughput and energy efficiency in topology-controlled multi-hop wireless sensor networks","http://dl.acm.org/citation.cfm?id=941350.941369","In the context of multi-hop wireless networks, various topology control algorithms have been proposed to adapt the transmission range of nodes based on local information while maintaining a connected topology. These algorithms are particularly suited for deployment in sensor networks which typically consist of energy constrained sensors. Sensor nodes should support power adaptation in order to use the benefits of topology control for energy conservation. In this paper, we design a framework for evaluating the performance of topology control algorithms using overall network throughput, and total energy consumption per packet delivered, as the metrics. Our goal is to identify the scenarios in which topology control improves the network performance. We supplement our analysis with ns2 simulations using the cone-based topology control algorithm [10, 19].Based on our analysis and simulations, we find that link layer retransmissions are essential with topology control to avoid throughput degradation due to increase in number of hops in lightly loaded networks. In heavily loaded networks, the throughput can be improved by a factor up to k2, where k is the average factor of reduction in transmission range using topology control. Studies of energy consumption reveal that improvements of up to $k^4$ can be obtained using topology control. However, these improvements decrease as the traffic pattern shifts from local (few hop connections) to non-local (hop lengths of the order of the diameter of the network). These results can be used to guide the deployment of topology control algorithms in sensor networks.","network"
"67","1080776.1080788","Names, addresses and identities in ambient networks","http://dl.acm.org/citation.cfm?id=1080776.1080788","Ambient Networks interconnect independent realms that may use different local network technologies and may belong to different administrative or legal entities. At the core of these advanced internetworking concepts is a flexible naming architecture based on dynamic indirections between names, addresses and identities. This paper gives an overview of the connectivity abstractions of Ambient Networks and then describes its naming architecture in detail, comparing and contrasting them to other related next-generation network architectures.","network"
"68","941350.941363","Information-directed routing in ad hoc sensor networks","http://dl.acm.org/citation.cfm?id=941350.941363","In a sensor network, data routing is tightly coupled to the needs of a sensing task, and hence the application semantics. This paper introduces the novel idea of information-directed routing, in which routing is formulated as a joint optimization of data transport and information aggregation. The routing objective is to minimize communication cost while maximizing information gain, differing from routing considerations for more general ad hoc networks. The paper uses the concrete problem of locating and tracking possibly moving signal sources as an example of information generation processes, and considers two common information extraction patterns in a sensor network: routing a user query from an arbitrary entry node to the vicinity of signal sources and back, or to a prespecified exit node, maximizing information accumulated along the path. We derive information constraints from realistic signal models, and present several routing algorithms that find near-optimal solutions for the joint optimization problem. Simulation results have demonstrated that information-directed routing is a significant improvement over a previously reported greedy algorithm, as measured by sensing quality such as localization and tracking accuracy and communication quality such as success rate in routing around sensor holes.","network"
"69","1236360.1236363","DESYNC: self-organizing desynchronization and TDMA on wireless sensor networks","http://dl.acm.org/citation.cfm?id=1236360.1236363","Desynchronization is a novel primitive for sensor networks: it implies that nodes perfectly interleave periodic events to occur in a round-robin schedule. This primitive can be used to evenly distribute sampling burden in a group of nodes, schedule sleep cycles, or organize a collision-free TDMA schedule for transmitting wireless messages. Here we present Desync a biologically-inspired self-maintaining algorithm for desynchronization in a single-hop network. We present (1) theoretical results showing convergence, (2) experimental results on TinyOS-based Telos sensor motes, and (3) a Desync based TDMA protocol. Desync-TDMA addresses two weaknesses of traditional TDMA: it does not require a global clock and it automatically adjusts to the number of participating nodes, so that bandwidth is always fully utilized. Experimental results show a reduction in message loss under high contention from approximately 58% to less than 1%, as well as a 25% increase in throughput over the default Telos MAC protocol.","network"
"70","1236360.1236370","Underground structure monitoring with wireless sensor networks","http://dl.acm.org/citation.cfm?id=1236360.1236370","Environment monitoring in coal mines is an important application of wireless sensor networks (WSNs) that has commercial potential. We discuss the design of a Structure-Aware Self-Adaptive WSN system, SASA. By regulating the mesh sensor network deployment and formulating a collaborative mechanism based on a regular beacon strategy, SASA is able to rapidly detect structure variations caused by underground collapses. A prototype is deployed with 27 Mica2 motes. We present our implementation experiences as well as the experimental results. To better evaluate the scalability and reliability of SASA, we also conduct a large-scale trace-driven simulation based on real data collected from the experiments.","network"
"72","984622.984627","The impact of spatial correlation on routing with compression in wireless sensor networks","http://dl.acm.org/citation.cfm?id=984622.984627","The efficacy of data aggregation in sensor networks is a function of the degree of spatial correlation in the sensed phenomenon. While several data aggregation (i.e., routing with data compression) techniques have been proposed in the literature, an understanding of the performance of various data aggregation schemes across the range of spatial correlations is lacking. We analyze the performance of routing with compression in wireless sensor networks using an application-independent measure of data compression (an empirically obtained approximation for the joint entropy of sources as a function of the distance between them) to quantify the size of compressed information, and a bit-hop metric to quantify the total cost of joint routing with compression. Analytical modeling and simulations reveal that while the nature of optimal routing with compression does depend on the correlation level, surprisingly, there exists a practical static clustering scheme which can provide near-optimal performance for a wide range of spatial correlations. This result is of great practical significance as it shows that a simple cluster-based system design can perform as well as sophisticated adaptive schemes for joint routing and compression.","network"
"73","776322.776346","A reputation system for peer-to-peer networks","http://dl.acm.org/citation.cfm?id=776322.776346","We investigate the design of a reputation system for decentralized unstructured P2P networks like Gnutella. Having reliable reputation information about peers can form the basis of an incentive system and can guide peers in their decision making (e.g., who to download a file from). The reputation system uses objective criteria to track each peer's contribution in the system and allows peers to store their reputations locally. Reputation are computed using either of the two schemes, debit-credit reputation computation (DCRC) and credit-only reputation computation (CORC). Using a reputation computation agent (RCA), we design a public key based mechanism that periodically updates the peer reputations in a secure, light-weight, and partially distributed manner. We evaluate using simulations the performance tradeoffs inherent in the design of our system.","network"
"74","1159681.1159689","Integrating network-calculus-based simulation with packet-level simulation for TCP-operated networks","http://dl.acm.org/citation.cfm?id=1159681.1159689","In this paper, we investigate the issue of integrating packet-level simulation with network-calculus-based simulation for TCP-operated networks in order to simultaneously achieve within one simulation framework the performance gains acquired by the latter and the packet-level details afforded by the former. In such mixed-mode simulation, a foreground flow is simulated at the packet-level while the other background flows are simulated in network-calculus-based simulation. One major challenge in realizing this notion is how to characterize the interaction between packet-level flows and network-calculus-based flows. In this paper, we develop the model of interaction at routers. We then implement mixed-mode simulation in ns-2, and conduct a simulation study to evaluate it in the perspective of efficiency in execution time and error discrepancy in simulation results.The simulation results indicate that mixed-mode simulation is an effective candidate for carrying out large-scale simulation for TCP networks since it makes significant performance improvement while retaining the packet-level details. Specifically, mixed-mode simulation achieves an order of magnitude or more improvement (maximally 20 times) in execution time and the improvement becomes more pronounced as the network size increases. On the other hand, the error discrepancy is within 1-2% of the bottleneck link capacity (e.g., the system throughput).","network"
"75","2721796.2721857","A multi-cast communication scheme using weak electrical current for intra-body networks","http://dl.acm.org/citation.cfm?id=2721796.2721857","Implantable medical devices have paved the way for realizing intra-body networks (IBNs) that are capable of communicating information from within the body. Traditional forms of RF-based based communication face drawbacks in terms of high absorption within tissues and can potentially result in security and privacy issues, owing to the omnidirectional radiation. Instead, we propose the use of weak electrical current as a means of intra-body communication for implants within the human body and to a relay and gateway node located on the surface of the skin that facilitates remote patient monitoring. The main contributions of this paper are: (i) signal reflection and refraction analysis of electromagnetic waves through human tissue boundaries, and (ii) the design of a combined multi-hop and multi-cast communication scheme for communication between implants. Our results reveal that a multi-cast communication scheme can be achieved for IBNs with the appropriate selection of transmission parameters.","network"
"76","1163928.1163935","Deployment and management of component-based services in active networks","http://dl.acm.org/citation.cfm?id=1163928.1163935","This paper describes an integrated architecture for deployment and management of component-based services on heterogeneous programmable and active network nodes. The approach allows for flexible handling of active services on a fine-grained level thanks to the full component orientation and the application of mature concepts commonly used in enterprise software systems. The main features of the proposed solution are: a hierarchical component service model, two-tiered service deployment architecture with separated network-level and node-level deployment requirement resolution engines, out-of-band node-level-controlled code distribution on demand, as well as deployment and management support for heterogeneous multi-plane distributed active services. The architecture has been implemented and experimentally evaluated within the FAIN project.","network"
"77","1127777.1127823","A linear programming approach to NLOS error mitigation in sensor networks","http://dl.acm.org/citation.cfm?id=1127777.1127823","In this paper, we propose a linear programming approach to the problem of non-line-of-sight (NLOS) error mitigation in sensor networks. The locations of sensor nodes can be estimated using range or distance estimates from location-aware 'anchor' nodes. In the absence of line-of-sight (LOS) between the sensor and anchor nodes, e.g., in indoor networks, the NLOS range estimates can be severely biased. If these biased range estimates are directly incorporated into practical location estimators such as the Least-Squares (LS) estimator without the mitigation of these bias errors, this can potentially lead to degradation in the accuracy of sensor location estimates. On the other hand, discarding the biased range estimates may not be a viable option, since the number of range estimates available may be limited. We present a novel NLOS bias mitigation scheme, based on linear programming, that (i) allows us to incorporate NLOS range information into sensor location-estimation, but (ii) does not allow NLOS bias errors to degrade sensor localization accuracy.","network"
"78","1066409.1066411","An engineering approach to dynamic prediction of network performance from application logs","http://dl.acm.org/citation.cfm?id=1066409.1066411","Network measurement traces contain information regarding network behavior over the period of observation. Research carried out from different contexts shows predictions of network behavior can be made depending on network past history. Existing works on network performance prediction use a complicated stochastic modeling approach that extrapolates past data to yield a rough estimate of long-term future network performance. However, prediction of network performance in the immediate future is still an unresolved problem. In this paper, we address network performance prediction as an engineering problem. The main contribution of this paper is to predict network performance dynamically for the immediate future. Our proposal also considers the practical implication of prediction. Therefore, instead of following the conventional approach to predict one single value, we predict a range within which network performance may lie. This range is bounded by our two newly proposed indices, namely, Optimistic Network Performance Index (ONPI) and Robust Network Performance Index (RNPI). Experiments carried out using one-year-long traffic traces between several pairs of real-life networks validate the usefulness of our model.","network"
"79","1460933.1461120","A user-centric and context-aware solution to interface management and access network selection in heterogeneous wireless environments","http://dl.acm.org/citation.cfm?id=1460933.1461120","This paper investigates issues related to handover management of mobile terminals in heterogeneous wireless environments. The mobile terminals are equipped with multiple wireless radio interfaces and have a limited lifetime battery. Seamless mobility and power utilization efficiency become two important aspects of the handover management. We propose in this work user-centric network selection, power-saving interface management and adaptive handover initiation solutions at the terminal side to support seamless terminal-initiated and terminal-controlled vertical handover. The proposed access network selection is situation-aware and application-aware to suit different communication contexts. It enables terminals to select the most suitable access network according to various access network characteristics. Multiple wireless interfaces of a terminal device are handled in both idle and active communication modes to optimize the power consumption. We also address an adaptive handover initiation scheme to assist the service continuity. We conduct simulations and analyses of the proposed solutions to show their suitability and their efficiency.","network"
"80","1498915.1498921","Trail: A distance-sensitive sensor network service for distributed object tracking","http://dl.acm.org/citation.cfm?id=1498915.1498921","Distributed observation and control of mobile objects via static wireless sensors demands timely information in a distance-sensitive manner: Information about closer objects is required more often and more quickly than that of farther objects. In this article, we present a wireless sensor network protocol, Trail, that supports distance-sensitive tracking of mobile objects for in-network subscribers upon demand. Trail achieves a find time that is linear in the distance from a subscriber to an object, via a distributed data structure that is updated only locally when the object moves. Notably, Trail does not partition the network into a hierarchy of clusters and clusterheads, and as a result Trail has lower maintenance costs, is more locally fault tolerant, and it better utilizes the network in terms of load balancing and minimizing the size of the data structure needed for tracking. Moreover, Trail is reliable and energy efficient, despite the network dynamics that are typical of wireless sensor networks. Trail can be refined by tuning certain parameters, thereby yielding a family of protocols that are suited for different application settings such as rate of queries, rate of updates, and network size. We evaluate the performance of Trail by analysis, simulations in a 90  90 sensor network, and experiments on 105 Mica2 nodes in the context of a pursuer-evader control application.","network"
"870","2403646.2403716","Study on data preprocessing for daylight climate data","http://dl.acm.org/citation.cfm?id=2403646.2403716","It is well konwn that the real-world data tend to exist many data quality problems such as incompleteness and nosiy data. Data preprocessing technology can improve data quality effectively and provide more reliable data for the next step. A data preprocessing approach for daylight climate data is presented in this paper according to the characteristics of this data. Then this approach is applied to the real-world data and the experimental results show that the approach can enhance the data quality effectively. Besides, the integration of the domain knowledge into data preprocessing is emphasized in this paper in order to make data preprocessing more effective and more targeted.","preprocessing data"
"871","2622687.2623065","Data preprocessing for anomaly based network intrusion detection: A review","http://dl.acm.org/citation.cfm?id=2622687.2623065","Data preprocessing is widely recognized as an important stage in anomaly detection. This paper reviews the data preprocessing techniques used by anomaly-based network intrusion detection systems (NIDS), concentrating on which aspects of the network traffic are analyzed, and what feature construction and selection methods have been used. Motivation for the paper comes from the large impact data preprocessing has on the accuracy and capability of anomaly-based NIDS. The review finds that many NIDS limit their view of network traffic to the TCP/IP packet headers. Time-based statistics can be derived from these headers to detect network scans, network worm behavior, and denial of service attacks. A number of other NIDS perform deeper inspection of request packets to detect attacks against network services and network applications. More recent approaches analyze full service responses to detect attacks targeting clients. The review covers a wide range of NIDS, highlighting which classes of attack are detectable by each of these approaches. Data preprocessing is found to predominantly rely on expert domain knowledge for identifying the most relevant parts of network traffic and for constructing the initial candidate set of traffic features. On the other hand, automated methods have been widely used for feature extraction to reduce data dimensionality, and feature selection to find the most relevant subset of features from this candidate set. The review shows a trend toward deeper packet inspection to construct more relevant features through targeted content parsing. These context sensitive features are required to detect current attacks.","preprocessing data"
"872","1929344.1929444","The high-activity parallel implementation of data preprocessing based on MapReduce","http://dl.acm.org/citation.cfm?id=1929344.1929444","Data preprocessing is an important and basic technique for data mining and machine learning. Due to the dramatic increasing of information, traditional data preprocessing techniques are time-consuming and not fit for processing mass data. In order to tackle this problem, we present parallel data preprocessing techniques based on MapReduce which is a programming model to implement parallelization easily. This paper gives the implementation details of the techniques including data integration, data cleaning, data normalization and so on. The proposed parallel techniques can deal with large-scale data (up to terabytes) efficiently. Our experimental results show considerable speedup performances with an increasing number of processors.","preprocessing data"
"873","1768306.1768328","A model PM for preprocessing and data mining proper process","http://dl.acm.org/citation.cfm?id=1768306.1768328","Data Mining, as defined in 1996 by Piatetsky-Shapiro ([1]) is a step (crucial, but a step nevertheless) in a KDD (Knowledge Discovery in Data Bases) process. The Piatetsky-Shapiro's definition states that the KDD process consists of the following steps: developing an understanding of the application domain, creating a target data set, choosing the data mining task i.e. deciding whether the goal of the KDD process is classification, regression, clustering, etc..., choosing the data mining algorithm(s), data preprocessing, data mining (DM), interpreting mined patterns, deciding if a re-iteration is needed, and consolidating discovered knowledge.","preprocessing data"
"874","2099994.2100018","On the preprocessing of mass spectrometry proteomics data","http://dl.acm.org/citation.cfm?id=2099994.2100018","Mass-Spectrometry (MS) based biological analysis is a powerful approach for discovering novel biomarkers or identifying patterns and associations in biological samples. Each value of a spectrum is composed of two measurements, m/Z (mass to charge ratio) and intensity. Even if data produced by mass spectrometers contains potentially huge amount of information, data are often affected by errors and noise due to sample preparation and instrument approximation. Preprocessing consists of (possibly) eliminating noise from spectra and identifying significant values (peaks). Preprocessing techniques need to be applied before performing analysis: cleaned spectra may then be analyzed by using data mining techniques or can be compared with known spectra in databases. This paper surveys different techniques for spectra preprocessing, working either on a single spectrum, or on an entire data set. We analyze preprocessing techniques aiming to correct intensity and m/Z values in order to: (i) reduce noise, (ii) reduce amount of data, and (iii) make spectra comparable.","preprocessing data"
"875","2639267.2639269","Data Preprocessing and Intelligent Data Analysis","http://dl.acm.org/citation.cfm?id=2639267.2639269","This paper first provides an overview of data preprocessing, focusing on problems of real world data. These are primarily problems that have to be carefully understood and solved before any data analysis process can start. The paper discusses in detail two main reasons for performing data preprocessing: i problems with the data and ii preparation for data analysis. The paper continues with details of data preprocessing techniques achieving each of the above mentioned objectives. A total of 14 techniques are discussed. Two examples of data preprocessing applications from two of the most data rich domains are given at the end. The applications are related to semiconductor manufacturing and aerospace domains where large amounts of data are available, and they are fairly reliable. Future directions and some challenges are discussed at the end.","preprocessing data"
"876","2613615","Biological Knowledge Discovery Handbook: Preprocessing, Mining and Postprocessing of Biological Data, 1st edition","http://dl.acm.org/citation.cfm?id=2613615","The first comprehensive overview of preprocessing, mining, and postprocessing of biological dataMolecular biology is undergoing exponential growth in both the volume and complexity of biological dataand knowledge discovery offers the capacity to automate complex search and data analysis tasks. This book presents a vast overview of the most recent developments on techniques and approaches in the field of biological knowledge discovery and data mining (KDD)providing in-depth fundamental and technical field information on the most important topics encountered.Written by top experts, Biological Knowledge Discovery Handbook: Preprocessing, Mining, and Postprocessing of Biological Data covers the three main phases of knowledge discovery (data preprocessing, data processingalso known as data miningand data postprocessing) and analyzes both verification systems and discovery systems.BIOLOGICAL DATA PREPROCESSINGPart A: Biological Data ManagementPart B: Biological Data ModelingPart C: Biological Feature ExtractionPart D Biological Feature SelectionBIOLOGICAL DATA MININGPart E: Regression Analysis of Biological DataPart F Biological Data ClusteringPart G: Biological Data ClassificationPart H: Association Rules Learning from Biological DataPart I: Text Mining and Application to Biological DataPart J: High-Performance Computing for Biological Data MiningCombining sound theory with practical applications in molecular biology, Biological Knowledge Discovery Handbook is ideal for courses in bioinformatics and biological KDD as well as for practitioners and professional researchers in computer science, life science, and mathematics.","preprocessing data"
"877","2010989.2010998","Data set preprocessing and transformation in a database system","http://dl.acm.org/citation.cfm?id=2010989.2010998","In general, there is a significant amount of data mining analysis performed outside a database system, which creates many data management issues. This article presents a summary of our experience and recommendations to compute data set preprocessing and transformation inside a database system (i.e. data cleaning, record selection, summarization, denormalization, variable creation, coding), which is the most time-consuming task in data mining projects. This aspect is largely ignored in the literature. We present practical issues, common solutions and lessons learned when preparing and transforming data sets with the SQL language, based on experience from real-life projects. We then provide specific guidelines to translate programs written in a traditional programming language into SQL statements. Based on successful real-life projects, we present time performance comparisons between SQL code running inside the database system and external data mining programs. We highlight which steps in data mining projects become faster when processed by the database system. More importantly, we identify advantages and disadvantages from a practical standpoint based on data mining users feedback.","preprocessing data"
"878","2081989.2082018","OntoDataClean: ontology-based integration and preprocessing of distributed data","http://dl.acm.org/citation.cfm?id=2081989.2082018","Within the knowledge discovery in databases (KDD) process, previous phases to data mining consume most of the time spent analysing data. Few research efforts have been carried out in theses steps compared to data mining, suggesting that new approaches and tools are needed to support the preparation of data. As regards, we present in this paper a new methodology of ontology-based KDD adopting a federated approach to database integration and retrieval. Within this model, an ontology-based system called OntoDataClean has been developed dealing with instance-level integration and data preprocessing. Within the OntoDataClean development, a preprocessing ontology was built to store the information about the required transformations. Various biomedical experiments were carried out, showing that data have been correctly transformed using the preprocessing ontology. Although OntoDataClean does not cover every possible data transformation, it suggests that ontologies are a suitable mechanism to improve quality in the various steps of KDD processes.","preprocessing data"
"879","2536360.2536368","Making queries tractable on big data with preprocessing: through the eyes of complexity theory","http://dl.acm.org/citation.cfm?id=2536360.2536368","A query class is traditionally considered tractable if there exists a polynomial-time (PTIME) algorithm to answer its queries. When it comes to big data, however, PTIME algorithms often become infeasible in practice. A traditional and effective approach to coping with this is to preprocess data off-line, so that queries in the class can be subsequently evaluated on the data efficiently. This paper aims to provide a formal foundation for this approach in terms of computational complexity. (1) We propose a set of -tractable queries, denoted by TQ0, to characterize classes of queries that can be answered in parallel poly-logarithmic time (NC) after PTIME preprocessing. (2) We show that several natural query classes are -tractable and are feasible on big data. (3) We also study a set TQ of query classes that can be effectively converted to -tractable queries by refactorizing its data and queries for preprocessing. We introduce a form of NC reductions to characterize such conversions. (4) We show that a natural query class is complete for TQ. (5) We also show that TQ0  P unless P = NC, i.e., the set TQ0 of all -tractable queries is properly contained in the set P of all PTIME queries. Nonetheless, TQ = P, i.e., all PTIME query classes can be made -tractable via proper refactorizations. This work is a step towards understanding the tractability of queries in the context of big data.","preprocessing data"
"880","1785934.1785998","Creation, population and preprocessing of experimental data sets for evaluation of applications for the semantic web","http://dl.acm.org/citation.cfm?id=1785934.1785998","In this paper we describe the process of experimental ontology data set creation. Such a semantically enhanced data set is needed in experimental evaluation of applications for the Semantic Web. Our research focuses on various levels of the process of data set creation - data acquisition using wrappers, data preprocessing on the ontology instance level and adjustment of the ontology according to the nature of the evaluation step. Web application aimed at clustering of ontology instances is utilized in the process of experimental evaluation, serving both as an example of an application and visual presentation of the experimental data set to the user.","preprocessing data"
"881","1992796.2043356","Desktop software for patch-clamp raw binary data conversion and preprocessing","http://dl.acm.org/citation.cfm?id=1992796.2043356","Since raw data recorded by patch-clamp systems are always stored in binary format, electrophysiologists may experience difficulties with patch clamp data preprocessing especially when they want to analyze by custom-designed algorithms. In this study, we present desktop software, called PCD Reader, which could be an effective and convenient solution for patch clamp data preprocessing for daily laboratory use. We designed a novel class module, called clsPulseData, to directly read the raw data along with the parameters recorded from HEKA instruments without any other program support. By a graphical user interface, raw binary data files can be converted into several kinds of ASCII text files for further analysis, with several preprocessing options. And the parameters can also be viewed, modified and exported into ASCII files by a user-friendly Explorer style window. The real-time data loading technique and optimized memory management programming makes PCDReader a fast and efficient tool. The compiled software along with the source code of the clsPulseData class module is freely available to academic and nonprofit users.","preprocessing data"
"882","1413814.1413820","Preprocessing enhancements to improve data mining algorithms","http://dl.acm.org/citation.cfm?id=1413814.1413820","Preprocessing is often required before using clustering or other data mining algorithms to analyse multivariate data sets. The approaches discussed in this paper are enhanced implementations of a preprocess that utilises an algorithm to cluster points in a data set based upon each attribute independently, resulting in additional information about the data points with respect to each of its dimensions. Noise, data boundaries, and likely representatives of data subsets can be more easily identified, thus significantly improving the performance of subsequent clustering or data mining algorithms by combining this additional information across all dimensions and querying the results.","preprocessing data"
"883","1707618.1707647","On preprocessing data for financial credit risk evaluation","http://dl.acm.org/citation.cfm?id=1707618.1707647","Financial credit-risk evaluation is among a class of problems known to be semi-structured, where not all variables that are used for decision-making are either known or captured without error. Machine learning has been successfully used for credit-evaluation decisions. However, blindly applying machine learning methods to financial credit risk evaluation data with minimal knowledge of data may not always lead to expected results. We present and evaluate some data and methodological considerations that are taken into account when using machine learning methods for these decisions. Specifically, we consider the effects of preprocessing of credit-risk evaluation data used as input for machine learning methods.","preprocessing data"
"884","2130744.2130767","Data preprocessing and kappa coefficient","http://dl.acm.org/citation.cfm?id=2130744.2130767","Data preprocessing is an essential step of the KDD process. It makes it possible to extract useful information from data. We propose two coefficients which respectively study the informational contribution of initial data in supervised learning and the intrinsic structure of initial data in not supervised one. These coefficients are based on Kappa coefficient. The confrontation of these two coefficients enables us to determine if feature construction is useful. We can present a system allowing the optimization of preprocessing step : feature selection is applied in all cases; then the two coefficients are calculated for the selected features. With the comparison of the two coefficients, we can decide the importance of feature construction.","preprocessing data"
"885","1248823.1249128","Active User-Based and Ontology-Based Web Log Data Preprocessing for Web Usage Mining","http://dl.acm.org/citation.cfm?id=1248823.1249128","User identification and session identification are two major steps in preprocessing web log data for web usage mining. This paper introduces a fast active user-based user identification algorithm with time complexity O(n). The algorithm uses both an IP address and a finite users' inactive time to identify different users in the web log. Website ontology is useful for identifying website structure and break points for browsing behavior. For session identification, we present an ontology-based method that utilizes the website structure and functionalities to identify different sessions.","preprocessing data"
"886","2704574.2705387","Data Preprocessing Method Based on User Characteristic of Interests for Web Log Mining","http://dl.acm.org/citation.cfm?id=2704574.2705387","Web log mining is the most important method in Web data mining, and data preprocessing is the primary work. In order to find more value access mode and reduce the data size from the Web, find the data of users and even between users, this paper puts forward a method of Web log data preprocessing based on user characteristic of interests, and then put forward some concepts such as user interest, user interest similarity. Finally, after some experiments, we can show the superiority and recommended value of this new method.","preprocessing data"
"887","1083820.1083965","A Data Envelopment Analysis-Based Approach for Data Preprocessing","http://dl.acm.org/citation.cfm?id=1083820.1083965","In this paper, we show how the data envelopment analysis (DEA) model might be useful to screen training data so a subset of examples that satisfy monotonicity property can be identified. Using real-world health care and software engineering data, managerial monotonicity assumption, and artificial neural network (ANN) as a forecasting model, we illustrate that DEA-based data screening of training data improves forecasting accuracy of an ANN.","preprocessing data"
"888","977242.977299","Advanced Data Preprocessing for Intersites Web Usage Mining","http://dl.acm.org/citation.cfm?id=977242.977299","In recent years, Web usage mining has emerged as a new field of data mining and gained increasing attention from both the business and research communities. A particular area of importance is data preprocessing for Intersites WUM. The proposed methodology for this process has two main objectives. The first is to use classical preprocessing (data fusion, data cleaning, and data structuration) to significantly reduce, but in a relevant manner, the size of the Web servers' log files. The second is to use advanced data preprocessing, which employs an extra step called data summarization to increase the quality of data obtained after classical preprocessing. To validate this methodology's efficiency, an experiment joined and analyzed log files from four related servers.","preprocessing data"
"889","1943774.1943997","Data mining and preprocessing application on component reports of an airline company in Turkey","http://dl.acm.org/citation.cfm?id=1943774.1943997","Risk and safety have always been important considerations in aviation. With the rapid growth in air travel, flight delays, cancellations and incidents/accidents have also dramatically increased in recent years (Nazeri & Jianping, 2002). There is a large amount of knowledge and data accumulation in aviation industry. These data could be stored in the form of pilot reports, maintenance reports, incident reports or delay reports. This paper focuses on different preprocessing and feature selection techniques applied on the 15 component reports of an airline company in Turkey to understand and clean the data set. Regression analysis, anomaly detection analysis, find dependencies and rough sets are used in this study in order to reduce the data set. Also the classification techniques of data mining are used to predict the warning level of the component as the class attribute. For this purpose Polyanalyst, SPSS Clementine, Minitab and Rosetta software tools are used. Find laws module of Polyanalyst is used to find the relations and information retrieval about the components warning level.","preprocessing data"
